#!/usr/bin/env python3.7
#
# encodertest.py [--encoder=<encoder>]
#                [--decoder=<decoder>]
#                [--test_file=<tests-json>]
#                [--checksums_file=<checksums-json>]
#                [--label=<label>]
#                [--input_dir=<dir>]
#                [--base_dir=<dir>]
#                [--progress=none|spinner|verbose]
#                [--sets=<string>,..]
#                [--single=<string>]
#                [--parallel=<number>]
#                [--generate_checksums_file=<file>]
#
# JSON schema:
#
# [ // Array of tests
#   {
#       "description" : <string>  // optional, human readable description
#       "name" : <string>         // identifier used to identity directory and files generated by test
#       "include-parameters" : <filename> // optional, start with parameters in named file then apply 'paramaters'
#       "parameters" : { ... }    // json dictionary of codec parameters
#       "sets" : [ <strings> ]    // optional, list  of sets that include this test - controller by  command line --sets=...
#       "generate_base" : <bool>  // optional, control whether base+ercon should be encoded for this test, overriding command line
#   }
#    ...
# ]
#
# NB: Some parameters are treated specially:
#
#   - input_file: width, height, fps, bit_depth and format are extracted from filename if present (but are overridden by expicit params)
#
#   - input_file, base,  base_recon: Input directory is prepended
#
import json
import csv
import xxhash,hashlib
import os, os.path
import re
import subprocess
import sys
import concurrent.futures

def platform_exe(path, name):
	"""Given a directory and an executable file name, return platform specifix path."""
	exesuffix = {"win32":".exe","linux":""}
	return os.path.join(path, name+exesuffix.get(sys.platform,""))


def prefix_path(d, f):
	if not os.path.isabs(v):
		return os.path.join(d, f)
	else:
		return f

def hash(filename, blocksize=65536):
	"""
	Compute hash of named file.
	"""
	hash = xxhash.xxh3_64()
	with open(filename, "rb") as f:
		for block in iter(lambda: f.read(blocksize), b""):
			hash.update(block)
	return hash.hexdigest()

def hash_md5(filename, blocksize=65536):
	"""
	Compute md5 hash of named file.
	"""
	hash = hashlib.md5()
	with open(filename, "rb") as f:
		for block in iter(lambda: f.read(blocksize), b""):
			hash.update(block)
	return hash.hexdigest()

IMAGE_NAME_PARAMS = [
	("width", re.compile("([0-9]+)x[0-9]+")),
	("height", re.compile("[0-9]+x([0-9]+)")),
	("fps", re.compile("([0-9]+)(fps|hz)")),
	("bit_depth", re.compile("([0-9]+)(bits?|bpp)")),
	("format", re.compile("^(420p|422p|yuv|yuyv|y|)$"))
]

def parse_vooya_filename(name):
	"""
	Extract vooya-ish image metadata from filename in form suitable for use as encoder parameters.
	"""
	params = {}
	for p in name.split("_"):
		for n,r in IMAGE_NAME_PARAMS:
			m = r.match(p)
			if m:
				params[n] = m.group(1)

	if 'bit_depth' in params:
		if 'format' not in params:
			params['format'] = "420p"
		params['format'] += params.pop('bit_depth')

	if 'format' in params and params['format'].startswith('4'):
		params['format'] = 'yuv' + params['format']

	if 'width' in params:
		params['width'] = int(params['width'])
	if 'height' in params:
		params['height'] = int(params['height'])
	if 'fps' in params:
		params['fps'] = int(params['fps'])

	return params

def create_opl_file(width, height, output):
	data = []
	log = open(f"{output}_encoder.log", "r")
	poc = 0
	for line in log:
		if line[:6] == "[MD5Y ":
			frame_data = [poc, width, height]
			frame_data.append(line[6:38])
			frame_data.append(line[46:78])
			frame_data.append(line[86:118])
			data.append(frame_data)
			poc += 1

	with open(f"{output}.opl", 'w', newline='') as csvfile:
		writer = csv.writer(csvfile)
		writer.writerow(["PicOrderCntVal", "pic_width_max", "pic_height_max", "md5_y", "md5_u", "md5_v"])
		writer.writerows(data)

def create_txt_file(width, height, output, description):
	sample_rate = width * height * 50
	if sample_rate <= 29410000:
		level = 1
	elif sample_rate <= 124560000:
		level = 2
	elif sample_rate <= 527650000:
		level = 3
	else:
		level = 4
	lines = [
		f"Bitstream file name: {output}.bit\n",
		f"Explanation of bitstream features: {description}\n",
		f"Profile: Main\n",
		f"Level: {level}.1\n",
		f"Max picture width: {width}\n",
		f"Max picture height: {height}\n",
		f"Picture rate: 50\n",
		f"LTM release version number used to generate the bitstream: LTM 5.1.1.4\n",
		f"Contact name and email: ???\n",
	]
	f = open(f"{output}.txt", 'w')
	f.writelines(lines)
	f.close()

def run_subprocess(title, args, logfile, scriptfile, show_progress):
	"""Run a comamnd as a subprocess - report progress and capture output (both stderr and stdout) to a log file."""

	with open(logfile, "wt") as log:
		sys.stderr.write(f"    {title}: Start\r")
		if show_progress:
			sys.stderr.write("\r")
		else:
			sys.stderr.write("\n")

		print(" ".join(args), file=log)

		with subprocess.Popen(args, bufsize=1, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
							  universal_newlines=True) as p:
			for n,l in enumerate(p.stdout):
				# Progress
				if show_progress == "spinner":
					progress_indicator = "|/-\\|/-\\"[n % 8]
					sys.stderr.write(f"    {title}: {progress_indicator}      \r")
				elif show_progress == "verbose":
					sys.stderr.write(l)
				# Log file
				print(l.strip(), file=log)

			if p.wait() != 0:
				sys.stderr.write(f"    {title}: FAILED\n")
				return False

	sys.stderr.write(f"    {title}: OK\n")

	# Write scripts
	if scriptfile:
		with open(scriptfile + ".sh", "wt") as f:
			f.write("#!/bin/bash\n")
			f.write("${LTM_PATH}" + os.path.basename(args[0]) + " " + " ".join(args[1:]))
			f.write("\n")

		with open(scriptfile + ".bat", "wt") as f:
			f.write("@echo off\n")
			f.write("%LTM_PATH%" + os.path.basename(args[0]) + ".exe " + " ".join(args[1:]))
			f.write("\n")

	return True

def test_encode_decode(number, encoder, decoder, input_dir, base_dir, test, default_parameters, label, update_checksums, show_progress, decode_only):
	"""
	Run an encode followed by decode of given file (using given params)
	Return true if recon & decoded yuv match required checksum.
	"""

	## Merge per test params and defaults
	parameters = {**test['parameters'], **default_parameters}

	# Are any parameters read from file?
	if 'include-parameters' in test:
		with open(test['include-parameters']) as f:
			include_params = json.load(f)
			parameters = { **include_params, **parameters }

	# Extract any parameters from input name
	input_file_params = parse_vooya_filename(parameters['input_file'])
	parameters = { **input_file_params, **parameters }

	# Supress base if this test needs to generate its own
	if test.get('generate_base', False):
		if 'base' in parameters:
			parameters.pop('base')
		if 'base_recon' in parameters:
			parameters.pop('base_recon')

	# Add input dirs
	if 'input_file' in parameters and not os.path.isabs(parameters['input_file']):
		parameters['input_file'] = os.path.abspath(os.path.join(input_dir, parameters['input_file']))

	if 'base' in parameters and parameters['base'] != "" and not os.path.isabs(parameters['base']):
		parameters['base'] = os.path.abspath(os.path.join(base_dir, parameters['base']))

	if 'base_recon' in parameters and parameters['base_recon'] != "" and not os.path.isabs(parameters['base_recon']):
		parameters['base_recon'] = os.path.abspath(os.path.join(base_dir, parameters['base_recon']))

	# Figure input name and output name
	basename = str(os.path.splitext(os.path.basename(parameters['input_file']))[0])
	output = f"test_{label}{test['name']}"

	# Change to a subdirectory for each test
	# Avoids any issues with parallel runs clashing over temp files.
	#
	current_dir = os.getcwd()
	if not os.path.exists(output):
		os.mkdir(output)
	if not os.path.isdir(output):
		print(f"Cannot make test directory:{output}", file=sys.stderr)
		return False

	os.chdir(output)

	# Convert paths back to relative
	# (All a bit of a dance - but means everything survives interesting filesystem layouts and is readable)
	#
	if 'input_file' in parameters:
		parameters['input_file'] = os.path.relpath(parameters['input_file'])

	if 'base' in parameters:
		parameters['base'] = os.path.relpath(parameters['base'])

	if 'base_recon' in parameters:
		parameters['base_recon'] = os.path.relpath(parameters['base_recon'])

	# Create the parameter file
	with open(output + ".json", "wt") as f:
		json.dump(parameters, f, indent=4)

	sys.stderr.write(f"Encode/Decode {number}: {label} {basename} : {test['description']}")
	sys.stderr.write("\n" + json.dumps(parameters, indent=4) + "\n")

	#
	encode_arguments = [
		os.path.relpath(encoder),
		f"--width={parameters['width']}",
		f"--height={parameters['height']}",
		f"--format={parameters['format']}",
		f"--base_encoder={parameters['base_encoder']}",
		f"--encapsulation={parameters['encapsulation']}",
		f"--parameters={output}.json",
		f"--output_file={output}.lvc",
		f"--output_recon={output}_recon.yuv",
		f"--keep_base=false",
		f"--parameter_config=conformance",
		f"--qp={parameters['qp']}" ]

	if 'limit' in test:
		encode_arguments.append(f"--limit={test['limit']}")

	#
	decode_arguments = [
		os.path.relpath(decoder),
		f"--base_encoder={parameters['base_encoder']}",
		f"--encapsulation={parameters['encapsulation']}",
		f"--input_file={output}.lvc",
		f"--output_file={output}_decoded.yuv" ]

	# Use fixed seed for dithering
	if 'dithering_control' in parameters:
		decode_arguments.append("--dithering_fixed=true")

	# Use external base decoder for monochrome input or 12/14 bpp
	if 'format' in parameters:
		if not parameters['format'].startswith('yuv'):
			decode_arguments.append("--base_external=true")
		elif parameters['format'].endswith('12') or parameters['format'].endswith('14'):
			decode_arguments.append("--base_external=true")

	# Encode
	if decode_only:
		encoder_ok = True
	else:
		encoder_ok = run_subprocess(f"  Encode {number}", encode_arguments,
									f"{output}_encoder.log", f"{output}_encode", show_progress)

	# Encode recon checksums
	if 	os.path.exists(f"{output}_recon.yuv"):
		encoded_checksum = hash(f"{output}_recon.yuv")
	else:
		encoded_checksum = "No encoded output";

	encoded_checksum_md5 = hash_md5(f"{output}_recon.yuv")

	# Encode bitstream checksums
	encoded_bitstream_checksum_md5 = hash_md5(f"{output}.lvc")

	# Decode
	decoder_ok = run_subprocess(f"  Decode {number}", decode_arguments,
								f"{output}_decoder.log", f"{output}_decode", show_progress)

	# Decode checksum
	if os.path.exists(f"{output}_decoded.yuv"):
		decoded_checksum = hash(f"{output}_decoded.yuv")
	else:
		decoded_checksum = "No decoded output";

	#
	test_checksum = test['checksums'].get(basename, "--------------------------------")

	# Record recon md5 checksum
	with open(output + ".yuv.md5", "wt") as f:
		f.write(encoded_checksum_md5 + "\n")

	# Record checksum status
	with open(output + "_status.txt", "wt") as f:
		f.write(f"encode:{encoded_checksum} decode:{decoded_checksum} reference:{test_checksum} ok:{encoded_checksum==decoded_checksum}\n")

	# Record recon md5 checksum
	with open(output + ".md5", "wt") as f:
		f.write(encoded_bitstream_checksum_md5 + "\n")

	check_encode = True
	if update_checksums:
		# Update checksum (keyed by input name)
		if 'checksums' not in test:
			test['checksums'] = {}
		test['checksums'][basename] = encoded_checksum
		print(f"Encoded checksum updated {number}", file=sys.stderr)
	elif 'checksums' in test and basename in test['checksums']:
		# Check against stored checksum
		check_encode = test['checksums'][basename] == encoded_checksum
		print(f" Encoded checksum {number}: ", check_encode and "OK" or "FAILED", file=sys.stderr)

	check_match = encoded_checksum == decoded_checksum
	print(f"Encoded vs decoded checksum {number}: ", check_match and "OK" or "FAILED", file=sys.stderr)

	# Option to remove yuv outputs
	# if check_match:
	# 	os.remove(output + "_recon.yuv")
	# 	os.remove(output + "_decoded.yuv")

	# Create .opl and .txt files as specified in MPEG Conformance Test
	if check_match:
		create_opl_file(parameters['width'], parameters['height'], output)
		create_txt_file(parameters['width'], parameters['height'], output, test['description'])

	# Rename UserData files
	if os.path.isfile("userdata_enc.bin"):
		os.rename("userdata_enc.bin", f"{output}_userdata_enc.bin")
	if os.path.isfile("userdata_dec.bin"):
		os.rename("userdata_dec.bin", f"{output}_userdata.bin")

	os.chdir(current_dir)

	return (encoder_ok and decoder_ok and check_encode and check_match)

def run_test(test_job):
	"""Wrapper for running a parallel test instance - unpacks arguments and forwards to real function."""
	print(f"-- Parallel Test {test_job['number']} {test_job['test']['description']}")
	ok = test_encode_decode(**test_job)
	return ok, test_job['number'], test_job['test']

def run_tests(args):
	"""Read a JSON array and run test described by each entry."""

	fails = 0

	# Load tests
	with open(args.test_file) as f:
		tests = json.load(f)

	if type(tests) != list:
		print("Top level of tests JSON must be an array.", file=sys.stderr)
		return 1

	default_parameters = json.loads(args.parameters)

	if args.sets:
		test_set = set(args.sets.split(','))
	else:
		test_set = set()

    ## Build test jobs - apply filters from command line
	test_jobs = []
	for n,test in enumerate(tests):

		# Single test
		if args.single and test['name'] != args.single:
			continue

		# Sets of tests
		if test_set:
			if 'sets' not in test or test_set.isdisjoint(set(test['sets'])):
				continue

		test_jobs.append({
			'number':n,
			'test':test,
			'encoder':os.path.abspath(args.encoder),
			'decoder':os.path.abspath(args.decoder),
			'input_dir':args.input_dir,
			'base_dir':args.base_dir,
			'default_parameters':default_parameters,
			'label':args.label,
			'update_checksums':args.updated_checksums != "",
			'show_progress': args.progress,
			'decode_only': args.decode_only})

	## Run the jobs
	if args.parallel == "0":
		print(f"Running {len(test_jobs)} tests", file=sys.stderr)
		# Run jobs serially in process
		for j in test_jobs:
			ok = test_encode_decode(**j)
			if not ok:
				fails += 1
	else:
		print(f"Running {len(test_jobs)} tests using {args.parallel} workers", file=sys.stderr)
		# Run jobs in parallel in seperate processes
		with concurrent.futures.ProcessPoolExecutor(max_workers=int(args.parallel)) as executor:
			for ok,n,updated_test in executor.map(run_test, test_jobs):
				if not ok:
					fails += 1
				if ok and args.updated_checksums:
					# Collect updated data from subprocess
					tests[n] = updated_test

	if fails != 0:
		print(f"Tests failed: {fails}", file=sys.stderr)

	## Dump new test data with updated checksums
	if args.updated_checksums:
		with open(args.updated_checksums, "wt") as f:
			json.dump(tests, f, indent=4)

	return fails

if __name__ == "__main__":
	import argparse
	parser = argparse.ArgumentParser()
	parser.add_argument("--label", help="label for test outputs", default="")
	parser.add_argument("--encoder", help="Test Model encoder", default=platform_exe("..","ModelEncoder"))
	parser.add_argument("--decoder", help="Test Model decoder", default=platform_exe("..","ModelDecoder"))
	parser.add_argument("--test_file", help="JSON file of tests", default="tests.json")
	parser.add_argument("--input_dir", help="Base for test inputs", default="inputs")
	parser.add_argument("--base_dir", help="Base for test input base and base recon.", default="bases")
	parser.add_argument("--parameters", help="Default parameters for all tests", default="{}")
	parser.add_argument("--progress", help="Display progresss spinner while coding", default="spinner")
	parser.add_argument("--updated_checksums", help="Write a new version of test file with updated checksums", default="")
	parser.add_argument("--single", help="Select single test to be run", default="")
	parser.add_argument("--sets", help="Select set of tests to be run", default="")
	parser.add_argument("--parallel", help="Number of parallel workers to use", default="0")
	parser.add_argument("--decode_only", help="Perform decoding only", default="")
	args = parser.parse_args()

	sys.exit(run_tests(args))
